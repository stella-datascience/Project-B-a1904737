import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from torch.utils.data import DataLoader
import pandas as pd
from tqdm import tqdm
import os

class ModelEvaluator:
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        
    def get_embeddings_and_predictions(self, dataloader):
        """Extract embeddings and predictions from the model"""
        self.model.ema_model.eval()
        
        all_embeddings = []
        all_predictions = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for data, targets in tqdm(dataloader, desc='Extracting embeddings'):
                data = data.to(self.device)
                
                # Get embeddings (output before final layer)
                features = self.model.ema_model.conv1(data)
                features = self.model.ema_model.block1(features)
                features = self.model.ema_model.block2(features)
                features = self.model.ema_model.block3(features)
                features = self.model.ema_model.relu(self.model.ema_model.bn1(features))
                features = F.avg_pool2d(features, 8)
                embeddings = features.view(features.size(0), -1)
                
                # Get predictions
                logits = self.model.ema_model.fc(embeddings)
                probs = torch.softmax(logits, dim=1)
                _, predictions = torch.max(logits, 1)
                
                all_embeddings.append(embeddings.cpu().numpy())
                all_predictions.append(predictions.cpu().numpy())
                all_targets.append(targets.numpy())
                all_probs.append(probs.cpu().numpy())
        
        return (np.concatenate(all_embeddings),
                np.concatenate(all_predictions),
                np.concatenate(all_targets),
                np.concatenate(all_probs))
    
    def plot_tsne(self, embeddings, targets, title="t-SNE Visualization", save_path=None):
        """Create t-SNE visualization of embeddings"""
        print("Computing t-SNE...")
        tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
        embeddings_2d = tsne.fit_transform(embeddings)
        
        plt.figure(figsize=(12, 10))
        scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], 
                             c=targets, cmap='tab10', alpha=0.6, s=10)
        plt.colorbar(scatter)
        plt.title(title, fontsize=16)
        plt.xlabel('t-SNE Component 1', fontsize=12)
        plt.ylabel('t-SNE Component 2', fontsize=12)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        return embeddings_2d
    
    def plot_pca(self, embeddings, targets, title="PCA Visualization", save_path=None):
        """Create PCA visualization of embeddings"""
        print("Computing PCA...")
        pca = PCA(n_components=2, random_state=42)
        embeddings_2d = pca.fit_transform(embeddings)
        
        plt.figure(figsize=(12, 10))
        scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], 
                             c=targets, cmap='tab10', alpha=0.6, s=10)
        plt.colorbar(scatter)
        plt.title(f'{title}\nExplained Variance: {pca.explained_variance_ratio_.sum():.3f}', fontsize=16)
        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.3f})', fontsize=12)
        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.3f})', fontsize=12)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        return embeddings_2d
    
    def plot_confusion_matrix(self, predictions, targets, class_names=None, 
                            normalize=True, save_path=None):
        """Plot confusion matrix"""
        if class_names is None:
            class_names = [f'Class {i}' for i in range(10)]
        
        cm = confusion_matrix(targets, predictions)
        
        if normalize:
            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            fmt = '.2f'
            title = 'Normalized Confusion Matrix'
        else:
            fmt = 'd'
            title = 'Confusion Matrix'
        
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues', 
                   xticklabels=class_names, yticklabels=class_names)
        plt.title(title, fontsize=16)
        plt.xlabel('Predicted Label', fontsize=12)
        plt.ylabel('True Label', fontsize=12)
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        return cm
    
    def plot_confidence_distribution(self, probs, targets, predictions, save_path=None):
        """Plot confidence distribution for correct and incorrect predictions"""
        correct_mask = predictions == targets
        correct_confidences = probs[correct_mask].max(axis=1)
        incorrect_confidences = probs[~correct_mask].max(axis=1)
        
        plt.figure(figsize=(12, 6))
        
        plt.subplot(1, 2, 1)
        plt.hist(correct_confidences, bins=50, alpha=0.7, label='Correct', color='green')
        plt.hist(incorrect_confidences, bins=50, alpha=0.7, label='Incorrect', color='red')
        plt.xlabel('Confidence')
        plt.ylabel('Frequency')
        plt.title('Confidence Distribution')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(1, 2, 2)
        box_data = [correct_confidences, incorrect_confidences]
        plt.boxplot(box_data, labels=['Correct', 'Incorrect'])
        plt.title('Confidence Box Plot')
        plt.ylabel('Confidence')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        return correct_confidences, incorrect_confidences
    
    def generate_classification_report(self, predictions, targets, class_names=None):
        """Generate detailed classification report"""
        if class_names is None:
            class_names = [f'Class {i}' for i in range(10)]
        
        report = classification_report(targets, predictions, 
                                     target_names=class_names, output_dict=True)
        
        # Print detailed report
        print("\n" + "="*60)
        print("Detailed Classification Report")
        print("="*60)
        print(classification_report(targets, predictions, target_names=class_names))
        
        # Create accuracy per class plot
        class_accuracy = []
        for i in range(len(class_names)):
            class_mask = targets == i
            if class_mask.sum() > 0:
                acc = (predictions[class_mask] == i).mean()
                class_accuracy.append(acc)
            else:
                class_accuracy.append(0)
        
        plt.figure(figsize=(12, 6))
        bars = plt.bar(range(len(class_names)), class_accuracy, color='skyblue', alpha=0.7)
        plt.axhline(y=np.mean(class_accuracy), color='red', linestyle='--', 
                   label=f'Average: {np.mean(class_accuracy):.3f}')
        plt.xlabel('Classes')
        plt.ylabel('Accuracy')
        plt.title('Accuracy per Class')
        plt.xticks(range(len(class_names)), class_names, rotation=45)
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Add value labels on bars
        for bar, acc in zip(bars, class_accuracy):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{acc:.3f}', ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        plt.savefig('evaluation_results/class_accuracy.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return report
    
    def plot_training_curves(self, training_history, save_path=None):
        """Plot training curves if history is available"""
        if not training_history:
            print("No training history available.")
            return
        
        epochs = range(1, len(training_history['train_loss']) + 1)
        
        plt.figure(figsize=(15, 5))
        
        plt.subplot(1, 3, 1)
        plt.plot(epochs, training_history['train_loss'], label='Train Loss')
        if 'val_loss' in training_history:
            plt.plot(epochs, training_history['val_loss'], label='Val Loss')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.title('Training Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(1, 3, 2)
        plt.plot(epochs, training_history['train_acc'], label='Train Acc')
        if 'val_acc' in training_history:
            plt.plot(epochs, training_history['val_acc'], label='Val Acc')
        plt.xlabel('Epochs')
        plt.ylabel('Accuracy')
        plt.title('Training Accuracy')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(1, 3, 3)
        if 'lr' in training_history:
            plt.plot(epochs, training_history['lr'], label='Learning Rate')
            plt.xlabel('Epochs')
            plt.ylabel('Learning Rate')
            plt.title('Learning Rate Schedule')
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

def create_evaluation_report(model, test_loader, class_names, save_dir='evaluation_results'):
    """Create comprehensive evaluation report"""
    os.makedirs(save_dir, exist_ok=True)
    
    evaluator = ModelEvaluator(model)
    
    print("Starting comprehensive evaluation...")
    
    # Get embeddings and predictions
    embeddings, predictions, targets, probs = evaluator.get_embeddings_and_predictions(test_loader)
    
    # Calculate overall accuracy
    accuracy = (predictions == targets).mean()
    print(f"\nOverall Test Accuracy: {accuracy:.4f}")
    
    # 1. t-SNE visualization
    print("\n1. Creating t-SNE visualization...")
    tsne_embeddings = evaluator.plot_tsne(embeddings, targets, 
                                        "t-SNE Visualization of CIFAR-10 Test Set",
                                        f"{save_dir}/tsne_visualization.png")
    
    # 2. PCA visualization
    print("\n2. Creating PCA visualization...")
    pca_embeddings = evaluator.plot_pca(embeddings, targets,
                                      "PCA Visualization of CIFAR-10 Test Set",
                                      f"{save_dir}/pca_visualization.png")
    
    # 3. Confusion matrix
    print("\n3. Creating confusion matrix...")
    cm = evaluator.plot_confusion_matrix(predictions, targets, class_names,
                                       save_path=f"{save_dir}/confusion_matrix.png")
    
    # 4. Confidence distribution
    print("\n4. Analyzing confidence distribution...")
    correct_conf, incorrect_conf = evaluator.plot_confidence_distribution(
        probs, targets, predictions, 
        save_path=f"{save_dir}/confidence_distribution.png")
    
    # 5. Detailed classification report
    print("\n5. Generating classification report...")
    report = evaluator.generate_classification_report(predictions, targets, class_names)
    
    # 6. Save numerical results
    save_numerical_results(accuracy, cm, report, correct_conf, incorrect_conf, save_dir)
    
    print(f"\nEvaluation complete! Results saved to '{save_dir}' directory.")
    
    return {
        'accuracy': accuracy,
        'confusion_matrix': cm,
        'classification_report': report,
        'embeddings': embeddings,
        'predictions': predictions,
        'targets': targets
    }

def save_numerical_results(accuracy, cm, report, correct_conf, incorrect_conf, save_dir):
    """Save numerical results to files"""
    # Save accuracy and basic metrics
    with open(f'{save_dir}/results_summary.txt', 'w') as f:
        f.write("FlexMatch Transfer Learning Results\n")
        f.write("=" * 50 + "\n")
        f.write(f"Overall Accuracy: {accuracy:.4f}\n\n")
        
        f.write("Per-class Accuracy:\n")
        for class_name, metrics in report.items():
            if class_name in ['accuracy', 'macro avg', 'weighted avg']:
                continue
            f.write(f"{class_name}: {metrics['precision']:.3f} precision, "
                   f"{metrics['recall']:.3f} recall, {metrics['f1-score']:.3f} f1-score\n")
        
        f.write(f"\nMacro Average F1: {report['macro avg']['f1-score']:.3f}\n")
        f.write(f"Weighted Average F1: {report['weighted avg']['f1-score']:.3f}\n")
        
        f.write(f"\nConfidence Analysis:\n")
        f.write(f"Correct predictions mean confidence: {correct_conf.mean():.3f}\n")
        f.write(f"Incorrect predictions mean confidence: {incorrect_conf.mean():.3f}\n")
        f.write(f"Confidence gap: {correct_conf.mean() - incorrect_conf.mean():.3f}\n")
    
    # Save confusion matrix as CSV
    cm_df = pd.DataFrame(cm)
    cm_df.columns = [f'Pred_{i}' for i in range(cm.shape[1])]
    cm_df.index = [f'True_{i}' for i in range(cm.shape[0])]
    cm_df.to_csv(f'{save_dir}/confusion_matrix.csv')
    
    # Save detailed classification report as CSV
    report_df = pd.DataFrame(report).transpose()
    report_df.to_csv(f'{save_dir}/classification_report.csv')
