{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "504b100c-45dd-444e-acfa-ce6c19e62bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter notebook mode...\n",
      "Available functions:\n",
      "1. run_complete_experiment() - Run full pipeline\n",
      "2. pretrain_on_cifar100() - Pre-train on CIFAR-100\n",
      "3. transfer_to_cifar10() - Transfer learning\n",
      "4. comprehensive_evaluation() - Evaluation with visualizations\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from flexmatch import FlexMatch\n",
    "from flexmatch_utils import get_cifar100_dataset, get_cifar10_datasets\n",
    "\n",
    "def pretrain_on_cifar100():\n",
    "    \"\"\"Pre-train model on CIFAR-100\"\"\"\n",
    "    print(\"Pre-training on CIFAR-100...\")\n",
    "    \n",
    "    # Load CIFAR-100 datasets\n",
    "    train_dataset, test_dataset = get_cifar100_dataset()\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Create model for CIFAR-100 (100 classes)\n",
    "    model = FlexMatch(num_classes=100, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Training loop\n",
    "    model.model.train()\n",
    "    optimizer = torch.optim.SGD(model.model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "    \n",
    "    best_acc = 0\n",
    "    for epoch in range(85):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(model.device), target.to(model.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model.model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_acc = evaluate_supervised(model.model, test_loader, 100)\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save(model.model.state_dict(), 'checkpoints/cifar100_pretrained.pth')\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch: {epoch+1}, Loss: {total_loss/len(train_loader):.4f}, '\n",
    "                  f'Train Acc: {100.*correct/total:.2f}%, Test Acc: {test_acc:.2f}%')\n",
    "    \n",
    "    print(f\"Pre-training completed! Best test accuracy: {best_acc:.2f}%\")\n",
    "    return model\n",
    "\n",
    "def evaluate_supervised(model, test_loader, num_classes):\n",
    "    \"\"\"Evaluate supervised model on test set\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    return 100. * correct / total\n",
    "\n",
    "def transfer_to_cifar10(pretrained_model_path=None):\n",
    "    \"\"\"Transfer learning from CIFAR-100 to CIFAR-10 using FlexMatch\"\"\"\n",
    "    print(\"Starting transfer learning to CIFAR-10...\")\n",
    "    \n",
    "    # Load CIFAR-10 datasets with 5% labeled data\n",
    "    labeled_dataset, unlabeled_dataset, unlabeled_dataset_strong, test_dataset = get_cifar10_datasets(labeled_ratio=0.05)\n",
    "    \n",
    "    print(f\"Labeled samples: {len(labeled_dataset)}\")\n",
    "    print(f\"Unlabeled samples: {len(unlabeled_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    labeled_loader = DataLoader(labeled_dataset, batch_size=64, shuffle=True, num_workers=2, drop_last=True)\n",
    "    unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=320, shuffle=True, num_workers=2, drop_last=True)\n",
    "    unlabeled_loader_strong = DataLoader(unlabeled_dataset_strong, batch_size=320, shuffle=True, num_workers=2, drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Create FlexMatch model for CIFAR-10\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = FlexMatch(num_classes=10, device=device, lambda_u=1.0, T=0.5, threshold=0.95)\n",
    "    \n",
    "    # Load pre-trained weights (except the final layer)\n",
    "    if pretrained_model_path and os.path.exists(pretrained_model_path):\n",
    "        print(\"Loading pre-trained weights...\")\n",
    "        pretrained_dict = torch.load(pretrained_model_path, map_location=device)\n",
    "        \n",
    "        # Get current model state dict\n",
    "        model_dict = model.model.state_dict()\n",
    "        \n",
    "        # Filter out final layer weights\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() \n",
    "                          if k in model_dict and 'fc' not in k and v.size() == model_dict[k].size()}\n",
    "        \n",
    "        # Update model state dict\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.model.load_state_dict(model_dict)\n",
    "        \n",
    "        # Also update EMA model\n",
    "        ema_dict = model.ema_model.state_dict()\n",
    "        ema_dict.update(pretrained_dict)\n",
    "        model.ema_model.load_state_dict(ema_dict)\n",
    "    \n",
    "    # Training loop with FlexMatch\n",
    "    num_epochs =40\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.model.train()\n",
    "        \n",
    "        # Create iterators for the loaders\n",
    "        labeled_iter = iter(labeled_loader)\n",
    "        unlabeled_iter = iter(unlabeled_loader)\n",
    "        unlabeled_strong_iter = iter(unlabeled_loader_strong)\n",
    "        \n",
    "        total_loss_l = 0\n",
    "        total_loss_u = 0\n",
    "        total_mask_ratio = 0\n",
    "        num_batches = min(len(labeled_loader), len(unlabeled_loader))\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            try:\n",
    "                labeled_batch = next(labeled_iter)\n",
    "                unlabeled_batch_weak = next(unlabeled_iter)\n",
    "                unlabeled_batch_strong = next(unlabeled_strong_iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            \n",
    "            # FlexMatch training step\n",
    "            metrics = model.train_step(labeled_batch, unlabeled_batch_weak, unlabeled_batch_strong)\n",
    "            \n",
    "            total_loss_l += metrics['loss_l']\n",
    "            total_loss_u += metrics['loss_u']\n",
    "            total_mask_ratio += metrics['mask_ratio']\n",
    "        \n",
    "        # Update learning rate\n",
    "        model.scheduler.step()\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "            test_acc = evaluate_flexmatch(model, test_loader)\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
    "                  f'Loss_l: {total_loss_l/num_batches:.4f}, '\n",
    "                  f'Loss_u: {total_loss_u/num_batches:.4f}, '\n",
    "                  f'Mask_ratio: {total_mask_ratio/num_batches:.4f}, '\n",
    "                  f'Test Acc: {test_acc:.2f}%')\n",
    "            \n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                model.save_model('checkpoints/flexmatch_best.pth')\n",
    "    \n",
    "    print(f\"Transfer learning completed! Best test accuracy: {best_acc:.2f}%\")\n",
    "    return model\n",
    "\n",
    "def evaluate_flexmatch(model, test_loader):\n",
    "    \"\"\"Evaluate FlexMatch model on test set\"\"\"\n",
    "    model.ema_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(model.device), target.to(model.device)\n",
    "            outputs = model.ema_model(data)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    return 100. * correct / total\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        \n",
    "    def get_embeddings_and_predictions(self, dataloader):\n",
    "        \"\"\"Extract embeddings and predictions from the model\"\"\"\n",
    "        self.model.ema_model.eval()\n",
    "        \n",
    "        all_embeddings = []\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        all_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, targets in tqdm(dataloader, desc='Extracting embeddings'):\n",
    "                data = data.to(self.device)\n",
    "                \n",
    "                # Get embeddings (output before final layer)\n",
    "                features = self.model.ema_model.conv1(data)\n",
    "                features = self.model.ema_model.block1(features)\n",
    "                features = self.model.ema_model.block2(features)\n",
    "                features = self.model.ema_model.block3(features)\n",
    "                features = self.model.ema_model.relu(self.model.ema_model.bn1(features))\n",
    "                features = F.avg_pool2d(features, 8)\n",
    "                embeddings = features.view(features.size(0), -1)\n",
    "                \n",
    "                # Get predictions\n",
    "                logits = self.model.ema_model.fc(embeddings)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                _, predictions = torch.max(logits, 1)\n",
    "                \n",
    "                all_embeddings.append(embeddings.cpu().numpy())\n",
    "                all_predictions.append(predictions.cpu().numpy())\n",
    "                all_targets.append(targets.numpy())\n",
    "                all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        return (np.concatenate(all_embeddings),\n",
    "                np.concatenate(all_predictions),\n",
    "                np.concatenate(all_targets),\n",
    "                np.concatenate(all_probs))\n",
    "    \n",
    "    def plot_tsne(self, embeddings, targets, title=\"t-SNE Visualization\", save_path=None):\n",
    "        \"\"\"Create t-SNE visualization of embeddings\"\"\"\n",
    "        print(\"Computing t-SNE...\")\n",
    "        # Sample a subset for faster t-SNE computation\n",
    "        if len(embeddings) > 5000:\n",
    "            indices = np.random.choice(len(embeddings), 5000, replace=False)\n",
    "            embeddings = embeddings[indices]\n",
    "            targets = targets[indices]\n",
    "            \n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "        embeddings_2d = tsne.fit_transform(embeddings)\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                             c=targets, cmap='tab10', alpha=0.6, s=10)\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title(title, fontsize=16)\n",
    "        plt.xlabel('t-SNE Component 1', fontsize=12)\n",
    "        plt.ylabel('t-SNE Component 2', fontsize=12)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return embeddings_2d\n",
    "    \n",
    "    def plot_pca(self, embeddings, targets, title=\"PCA Visualization\", save_path=None):\n",
    "        \"\"\"Create PCA visualization of embeddings\"\"\"\n",
    "        print(\"Computing PCA...\")\n",
    "        pca = PCA(n_components=2, random_state=42)\n",
    "        embeddings_2d = pca.fit_transform(embeddings)\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                             c=targets, cmap='tab10', alpha=0.6, s=10)\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title(f'{title}\\nExplained Variance: {pca.explained_variance_ratio_.sum():.3f}', fontsize=16)\n",
    "        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.3f})', fontsize=12)\n",
    "        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.3f})', fontsize=12)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return embeddings_2d\n",
    "    \n",
    "    def plot_confusion_matrix(self, predictions, targets, class_names=None, \n",
    "                            normalize=True, save_path=None):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        if class_names is None:\n",
    "            class_names = [f'Class {i}' for i in range(10)]\n",
    "        \n",
    "        cm = confusion_matrix(targets, predictions)\n",
    "        \n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            fmt = '.2f'\n",
    "            title = 'Normalized Confusion Matrix'\n",
    "        else:\n",
    "            fmt = 'd'\n",
    "            title = 'Confusion Matrix'\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues', \n",
    "                   xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(title, fontsize=16)\n",
    "        plt.xlabel('Predicted Label', fontsize=12)\n",
    "        plt.ylabel('True Label', fontsize=12)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "    def plot_confidence_distribution(self, probs, targets, predictions, save_path=None):\n",
    "        \"\"\"Plot confidence distribution for correct and incorrect predictions\"\"\"\n",
    "        correct_mask = predictions == targets\n",
    "        correct_confidences = probs[correct_mask].max(axis=1)\n",
    "        incorrect_confidences = probs[~correct_mask].max(axis=1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(correct_confidences, bins=50, alpha=0.7, label='Correct', color='green')\n",
    "        plt.hist(incorrect_confidences, bins=50, alpha=0.7, label='Incorrect', color='red')\n",
    "        plt.xlabel('Confidence')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Confidence Distribution')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        box_data = [correct_confidences, incorrect_confidences]\n",
    "        plt.boxplot(box_data, labels=['Correct', 'Incorrect'])\n",
    "        plt.title('Confidence Box Plot')\n",
    "        plt.ylabel('Confidence')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return correct_confidences, incorrect_confidences\n",
    "    \n",
    "    def generate_classification_report(self, predictions, targets, class_names=None):\n",
    "        \"\"\"Generate detailed classification report\"\"\"\n",
    "        if class_names is None:\n",
    "            class_names = [f'Class {i}' for i in range(10)]\n",
    "        \n",
    "        report = classification_report(targets, predictions, \n",
    "                                     target_names=class_names, output_dict=True)\n",
    "        \n",
    "        # Print detailed report\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Detailed Classification Report\")\n",
    "        print(\"=\"*60)\n",
    "        print(classification_report(targets, predictions, target_names=class_names))\n",
    "        \n",
    "        # Create accuracy per class plot\n",
    "        class_accuracy = []\n",
    "        for i in range(len(class_names)):\n",
    "            class_mask = targets == i\n",
    "            if class_mask.sum() > 0:\n",
    "                acc = (predictions[class_mask] == i).mean()\n",
    "                class_accuracy.append(acc)\n",
    "            else:\n",
    "                class_accuracy.append(0)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(range(len(class_names)), class_accuracy, color='skyblue', alpha=0.7)\n",
    "        plt.axhline(y=np.mean(class_accuracy), color='red', linestyle='--', \n",
    "                   label=f'Average: {np.mean(class_accuracy):.3f}')\n",
    "        plt.xlabel('Classes')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy per Class')\n",
    "        plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars, class_accuracy):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{acc:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('evaluation_results/class_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return report\n",
    "\n",
    "def plot_sample_predictions(model, test_dataset, class_names, num_samples=20, save_path=None):\n",
    "    \"\"\"Plot sample predictions with true and predicted labels\"\"\"\n",
    "    model.ema_model.eval()\n",
    "    \n",
    "    # Create a data loader with batch size = num_samples\n",
    "    sample_loader = torch.utils.data.DataLoader(test_dataset, batch_size=num_samples, shuffle=True)\n",
    "    data_iter = iter(sample_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        images = images.to(model.device)\n",
    "        outputs = model.ema_model(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        probabilities = torch.softmax(outputs, 1)\n",
    "        confidences = probabilities.max(1)[0]\n",
    "    \n",
    "    images = images.cpu()\n",
    "    predictions = predictions.cpu()\n",
    "    labels = labels.cpu()\n",
    "    confidences = confidences.cpu()\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx in range(num_samples):\n",
    "        image = images[idx]\n",
    "        \n",
    "        # Denormalize for visualization\n",
    "        mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "        image = image * std + mean\n",
    "        image = torch.clamp(image, 0, 1)\n",
    "        \n",
    "        # Convert to HWC for matplotlib\n",
    "        image = image.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        pred_class = class_names[predictions[idx]]\n",
    "        true_class = class_names[labels[idx]]\n",
    "        confidence = confidences[idx]\n",
    "        \n",
    "        correct = predictions[idx] == labels[idx]\n",
    "        color = 'green' if correct else 'red'\n",
    "        \n",
    "        axes[idx].imshow(image)\n",
    "        axes[idx].set_title(f'True: {true_class}\\nPred: {pred_class}\\nConf: {confidence:.3f}', \n",
    "                          color=color, fontsize=10)\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def create_evaluation_report(model, test_loader, class_names, save_dir='evaluation_results'):\n",
    "    \"\"\"Create comprehensive evaluation report\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    evaluator = ModelEvaluator(model)\n",
    "    \n",
    "    print(\"Starting comprehensive evaluation...\")\n",
    "    \n",
    "    # Get embeddings and predictions\n",
    "    embeddings, predictions, targets, probs = evaluator.get_embeddings_and_predictions(test_loader)\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    accuracy = (predictions == targets).mean()\n",
    "    print(f\"\\nOverall Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # 1. t-SNE visualization\n",
    "    print(\"\\n1. Creating t-SNE visualization...\")\n",
    "    tsne_embeddings = evaluator.plot_tsne(embeddings, targets, \n",
    "                                        \"t-SNE Visualization of CIFAR-10 Test Set\",\n",
    "                                        f\"{save_dir}/tsne_visualization.png\")\n",
    "    \n",
    "    # 2. PCA visualization\n",
    "    print(\"\\n2. Creating PCA visualization...\")\n",
    "    pca_embeddings = evaluator.plot_pca(embeddings, targets,\n",
    "                                      \"PCA Visualization of CIFAR-10 Test Set\",\n",
    "                                      f\"{save_dir}/pca_visualization.png\")\n",
    "    \n",
    "    # 3. Confusion matrix\n",
    "    print(\"\\n3. Creating confusion matrix...\")\n",
    "    cm = evaluator.plot_confusion_matrix(predictions, targets, class_names,\n",
    "                                       save_path=f\"{save_dir}/confusion_matrix.png\")\n",
    "    \n",
    "    # 4. Confidence distribution\n",
    "    print(\"\\n4. Analyzing confidence distribution...\")\n",
    "    correct_conf, incorrect_conf = evaluator.plot_confidence_distribution(\n",
    "        probs, targets, predictions, \n",
    "        save_path=f\"{save_dir}/confidence_distribution.png\")\n",
    "    \n",
    "    # 5. Detailed classification report\n",
    "    print(\"\\n5. Generating classification report...\")\n",
    "    report = evaluator.generate_classification_report(predictions, targets, class_names)\n",
    "    \n",
    "    # 6. Save numerical results\n",
    "    save_numerical_results(accuracy, cm, report, correct_conf, incorrect_conf, save_dir)\n",
    "    \n",
    "    print(f\"\\nEvaluation complete! Results saved to '{save_dir}' directory.\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': report,\n",
    "        'embeddings': embeddings,\n",
    "        'predictions': predictions,\n",
    "        'targets': targets\n",
    "    }\n",
    "\n",
    "def save_numerical_results(accuracy, cm, report, correct_conf, incorrect_conf, save_dir):\n",
    "    \"\"\"Save numerical results to files\"\"\"\n",
    "    # Save accuracy and basic metrics\n",
    "    with open(f'{save_dir}/results_summary.txt', 'w') as f:\n",
    "        f.write(\"FlexMatch Transfer Learning Results\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        f.write(f\"Overall Accuracy: {accuracy:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Per-class Accuracy:\\n\")\n",
    "        for class_name, metrics in report.items():\n",
    "            if class_name in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                continue\n",
    "            f.write(f\"{class_name}: {metrics['precision']:.3f} precision, \"\n",
    "                   f\"{metrics['recall']:.3f} recall, {metrics['f1-score']:.3f} f1-score\\n\")\n",
    "        \n",
    "        f.write(f\"\\nMacro Average F1: {report['macro avg']['f1-score']:.3f}\\n\")\n",
    "        f.write(f\"Weighted Average F1: {report['weighted avg']['f1-score']:.3f}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nConfidence Analysis:\\n\")\n",
    "        f.write(f\"Correct predictions mean confidence: {correct_conf.mean():.3f}\\n\")\n",
    "        f.write(f\"Incorrect predictions mean confidence: {incorrect_conf.mean():.3f}\\n\")\n",
    "        f.write(f\"Confidence gap: {correct_conf.mean() - incorrect_conf.mean():.3f}\\n\")\n",
    "    \n",
    "    # Save confusion matrix as CSV\n",
    "    cm_df = pd.DataFrame(cm)\n",
    "    cm_df.columns = [f'Pred_{i}' for i in range(cm.shape[1])]\n",
    "    cm_df.index = [f'True_{i}' for i in range(cm.shape[0])]\n",
    "    cm_df.to_csv(f'{save_dir}/confusion_matrix.csv')\n",
    "    \n",
    "    # Save detailed classification report as CSV\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_df.to_csv(f'{save_dir}/classification_report.csv')\n",
    "\n",
    "def comprehensive_evaluation(model_path='checkpoints/flexmatch_best.pth'):\n",
    "    \"\"\"Comprehensive evaluation with visualizations\"\"\"\n",
    "    print(\"Starting comprehensive evaluation...\")\n",
    "    \n",
    "    # Load model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = FlexMatch(num_classes=10, device=device)\n",
    "    model.load_model(model_path)\n",
    "    \n",
    "    # Load test dataset\n",
    "    _, _, _, test_dataset = get_cifar10_datasets(labeled_ratio=0.05)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # CIFAR-10 class names\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                  'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    # Create evaluation report\n",
    "    results = create_evaluation_report(model, test_loader, class_names)\n",
    "    \n",
    "    # Additional visualizations\n",
    "    print(\"\\nGenerating sample predictions...\")\n",
    "    plot_sample_predictions(model, test_dataset, class_names, \n",
    "                          save_path='evaluation_results/sample_predictions.png')\n",
    "    \n",
    "    print(\"\\nComprehensive evaluation completed!\")\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Check if we're in a Jupyter environment\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        print(\"Running in Jupyter notebook mode...\")\n",
    "        print(\"Available functions:\")\n",
    "        print(\"1. run_complete_experiment() - Run full pipeline\")\n",
    "        print(\"2. pretrain_on_cifar100() - Pre-train on CIFAR-100\")\n",
    "        print(\"3. transfer_to_cifar10() - Transfer learning\")\n",
    "        print(\"4. comprehensive_evaluation() - Evaluation with visualizations\")\n",
    "        return\n",
    "    \n",
    "    # Command line mode\n",
    "    parser = argparse.ArgumentParser(description='FlexMatch Transfer Learning')\n",
    "    parser.add_argument('--pretrain', action='store_true', help='Pre-train on CIFAR-100')\n",
    "    parser.add_argument('--transfer', action='store_true', help='Transfer to CIFAR-10')\n",
    "    parser.add_argument('--evaluate', action='store_true', help='Run comprehensive evaluation')\n",
    "    parser.add_argument('--complete', action='store_true', help='Run complete experiment (all steps)')\n",
    "    parser.add_argument('--pretrained_path', type=str, default='checkpoints/cifar100_pretrained.pth', \n",
    "                       help='Path to pre-trained model')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.complete:\n",
    "        success = run_complete_experiment()\n",
    "        exit(0 if success else 1)\n",
    "    \n",
    "    if args.pretrain:\n",
    "        pretrain_on_cifar100()\n",
    "    \n",
    "    if args.transfer:\n",
    "        if not os.path.exists(args.pretrained_path) and args.pretrained_path == 'checkpoints/cifar100_pretrained.pth':\n",
    "            print(\"No pre-trained model found. Pre-training first...\")\n",
    "            pretrain_on_cifar100()\n",
    "        \n",
    "        transfer_to_cifar10(args.pretrained_path)\n",
    "    \n",
    "    if args.evaluate:\n",
    "        comprehensive_evaluation()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e52d98-f036-49c0-94c8-640c19126c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Step 1: Pre-training on CIFAR-100...\n",
      "Pre-training on CIFAR-100...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch: 5, Loss: 2.8792, Train Acc: 27.64%, Test Acc: 28.93%\n",
      "Epoch: 10, Loss: 2.1917, Train Acc: 41.82%, Test Acc: 42.34%\n",
      "Epoch: 15, Loss: 1.8699, Train Acc: 49.08%, Test Acc: 50.23%\n",
      "Epoch: 20, Loss: 1.7163, Train Acc: 52.82%, Test Acc: 48.59%\n",
      "Epoch: 25, Loss: 1.6238, Train Acc: 54.86%, Test Acc: 54.48%\n",
      "Epoch: 30, Loss: 1.5661, Train Acc: 56.24%, Test Acc: 53.67%\n",
      "Epoch: 35, Loss: 1.5239, Train Acc: 57.27%, Test Acc: 53.12%\n",
      "Epoch: 40, Loss: 1.4691, Train Acc: 58.59%, Test Acc: 55.65%\n",
      "Epoch: 45, Loss: 1.4358, Train Acc: 59.52%, Test Acc: 58.02%\n",
      "Epoch: 50, Loss: 1.3964, Train Acc: 60.81%, Test Acc: 57.07%\n",
      "Epoch: 55, Loss: 1.3618, Train Acc: 61.37%, Test Acc: 54.31%\n",
      "Epoch: 60, Loss: 1.3399, Train Acc: 61.89%, Test Acc: 58.87%\n",
      "Epoch: 65, Loss: 1.3256, Train Acc: 62.34%, Test Acc: 58.19%\n",
      "Epoch: 70, Loss: 1.2681, Train Acc: 63.64%, Test Acc: 60.33%\n",
      "Epoch: 75, Loss: 1.2214, Train Acc: 64.65%, Test Acc: 59.99%\n",
      "Epoch: 80, Loss: 1.1925, Train Acc: 65.63%, Test Acc: 60.69%\n",
      "Epoch: 85, Loss: 1.1523, Train Acc: 66.55%, Test Acc: 59.21%\n",
      "Pre-training completed! Best test accuracy: 60.99%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<flexmatch.FlexMatch at 0x24a543ce180>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nðŸš€ Step 1: Pre-training on CIFAR-100...\")\n",
    "pretrain_on_cifar100()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19de8e95-0bea-4db9-953f-83b931d54862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Step 2: Transfer learning to CIFAR-10 with FlexMatch...\n",
      "Starting transfer learning to CIFAR-10...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Labeled samples: 2500\n",
      "Unlabeled samples: 47500\n",
      "Test samples: 10000\n",
      "Loading pre-trained weights...\n",
      "Epoch [5/40] - Loss_l: 1.2667, Loss_u: 1.2054, Mask_ratio: 0.4204, Test Acc: 31.53%\n",
      "Epoch [10/40] - Loss_l: 0.9941, Loss_u: 1.3792, Mask_ratio: 0.4879, Test Acc: 49.80%\n",
      "Epoch [15/40] - Loss_l: 0.7482, Loss_u: 1.6535, Mask_ratio: 0.6440, Test Acc: 56.93%\n",
      "Epoch [20/40] - Loss_l: 0.5422, Loss_u: 1.8096, Mask_ratio: 0.7426, Test Acc: 60.31%\n",
      "Epoch [25/40] - Loss_l: 0.3075, Loss_u: 1.9272, Mask_ratio: 0.8236, Test Acc: 62.11%\n",
      "Epoch [30/40] - Loss_l: 0.2006, Loss_u: 1.9661, Mask_ratio: 0.8417, Test Acc: 63.46%\n",
      "Epoch [35/40] - Loss_l: 0.4083, Loss_u: 1.8831, Mask_ratio: 0.7745, Test Acc: 63.66%\n",
      "Epoch [40/40] - Loss_l: 0.1226, Loss_u: 2.0060, Mask_ratio: 0.8623, Test Acc: 62.03%\n",
      "Transfer learning completed! Best test accuracy: 63.66%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸš€ Step 2: Transfer learning to CIFAR-10 with FlexMatch...\")\n",
    "model = transfer_to_cifar10('checkpoints/cifar100_pretrained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c707b584-3d9d-494f-aa77-80543846e447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Step 3: Comprehensive Evaluation and Visualization...\n",
      "Starting comprehensive evaluation...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Starting comprehensive evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:18<00:00,  4.34it/s]\n",
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Test Accuracy: 0.6366\n",
      "\n",
      "1. Creating t-SNE visualization...\n",
      "Computing t-SNE...\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Comprehensive evaluation\n",
    "print(\"\\nðŸš€ Step 3: Comprehensive Evaluation and Visualization...\")\n",
    "results = comprehensive_evaluation('checkpoints/flexmatch_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770186d6-1329-4243-89fc-448bb8fbb6de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8150dcb5-d25b-4335-ba1a-8c43749f0a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe8f18d-3c1d-489d-8e20-e5b3308561eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
